FROM apache/airflow:2.9.0

# Install Python dependencies
RUN pip install --no-cache-dir scikit-learn pandas numpy kaggle joblib

# Create necessary custom data directories.
# The airflow user owns AIRFLOW_HOME (/opt/airflow), so it can create subdirectories here.
RUN mkdir -p ${AIRFLOW_HOME}/data/raw ${AIRFLOW_HOME}/data/processed

# Copy application code.
# Using --chown=airflow:airflow is explicit and robust.
COPY --chown=airflow:airflow ./src/ ${AIRFLOW_HOME}/src/
COPY --chown=airflow:airflow ./dags/ ${AIRFLOW_HOME}/dags/

# Copy the run script and make it executable.
# The script is copied to the root directory of the image here.
COPY --chown=airflow:airflow ./airflow_run.sh /airflow_run.sh
RUN chmod +x /airflow_run.sh # The airflow user, being the owner, can set execute permissions.

ENTRYPOINT ["/airflow_run.sh"]